//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 7.0.1
//

.version 9.1
.target sm_89
.address_size 64

	// .globl	_Z14addNaiveKerneliPKfPf

.visible .entry _Z14addNaiveKerneliPKfPf(
	.param .u32 _Z14addNaiveKerneliPKfPf_param_0,
	.param .u64 _Z14addNaiveKerneliPKfPf_param_1,
	.param .u64 _Z14addNaiveKerneliPKfPf_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<20>;


	ld.param.u32 	%r10, [_Z14addNaiveKerneliPKfPf_param_0];
	ld.param.u64 	%rd13, [_Z14addNaiveKerneliPKfPf_param_1];
	ld.param.u64 	%rd14, [_Z14addNaiveKerneliPKfPf_param_2];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	setp.lt.s32 	%p1, %r10, 1;
	@%p1 bra 	$L__BB0_7;

	add.s32 	%r12, %r10, -1;
	and.b32  	%r17, %r10, 3;
	setp.lt.u32 	%p2, %r12, 3;
	mov.u32 	%r16, 0;
	@%p2 bra 	$L__BB0_4;

	sub.s32 	%r15, %r10, %r17;
	mov.u32 	%r16, 0;
	mov.u64 	%rd16, %rd1;
	mov.u64 	%rd17, %rd2;

$L__BB0_3:
	ld.global.f32 	%f1, [%rd16];
	ld.global.f32 	%f2, [%rd17];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd16], %f3;
	ld.global.f32 	%f4, [%rd16+4];
	ld.global.f32 	%f5, [%rd17+4];
	add.f32 	%f6, %f5, %f4;
	st.global.f32 	[%rd16+4], %f6;
	ld.global.f32 	%f7, [%rd16+8];
	ld.global.f32 	%f8, [%rd17+8];
	add.f32 	%f9, %f8, %f7;
	st.global.f32 	[%rd16+8], %f9;
	ld.global.f32 	%f10, [%rd16+12];
	ld.global.f32 	%f11, [%rd17+12];
	add.f32 	%f12, %f11, %f10;
	st.global.f32 	[%rd16+12], %f12;
	add.s32 	%r16, %r16, 4;
	add.s64 	%rd17, %rd17, 16;
	add.s64 	%rd16, %rd16, 16;
	add.s32 	%r15, %r15, -4;
	setp.ne.s32 	%p3, %r15, 0;
	@%p3 bra 	$L__BB0_3;

$L__BB0_4:
	setp.eq.s32 	%p4, %r17, 0;
	@%p4 bra 	$L__BB0_7;

	mul.wide.s32 	%rd15, %r16, 4;
	add.s64 	%rd19, %rd1, %rd15;
	add.s64 	%rd18, %rd2, %rd15;

$L__BB0_6:
	.pragma "nounroll";
	ld.global.f32 	%f13, [%rd19];
	ld.global.f32 	%f14, [%rd18];
	add.f32 	%f15, %f14, %f13;
	st.global.f32 	[%rd19], %f15;
	add.s64 	%rd19, %rd19, 4;
	add.s64 	%rd18, %rd18, 4;
	add.s32 	%r17, %r17, -1;
	setp.ne.s32 	%p5, %r17, 0;
	@%p5 bra 	$L__BB0_6;

$L__BB0_7:
	ret;

}
	// .globl	_Z22addNaiveRestrictKerneliPKfPf
.visible .entry _Z22addNaiveRestrictKerneliPKfPf(
	.param .u32 _Z22addNaiveRestrictKerneliPKfPf_param_0,
	.param .u64 _Z22addNaiveRestrictKerneliPKfPf_param_1,
	.param .u64 _Z22addNaiveRestrictKerneliPKfPf_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<20>;


	ld.param.u32 	%r10, [_Z22addNaiveRestrictKerneliPKfPf_param_0];
	ld.param.u64 	%rd13, [_Z22addNaiveRestrictKerneliPKfPf_param_1];
	ld.param.u64 	%rd14, [_Z22addNaiveRestrictKerneliPKfPf_param_2];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	setp.lt.s32 	%p1, %r10, 1;
	@%p1 bra 	$L__BB1_7;

	add.s32 	%r12, %r10, -1;
	and.b32  	%r17, %r10, 3;
	setp.lt.u32 	%p2, %r12, 3;
	mov.u32 	%r16, 0;
	@%p2 bra 	$L__BB1_4;

	sub.s32 	%r15, %r10, %r17;
	mov.u32 	%r16, 0;
	mov.u64 	%rd16, %rd1;
	mov.u64 	%rd17, %rd2;

$L__BB1_3:
	ld.global.f32 	%f1, [%rd16];
	ld.global.nc.f32 	%f2, [%rd17];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd16], %f3;
	ld.global.f32 	%f4, [%rd16+4];
	ld.global.nc.f32 	%f5, [%rd17+4];
	add.f32 	%f6, %f5, %f4;
	st.global.f32 	[%rd16+4], %f6;
	ld.global.f32 	%f7, [%rd16+8];
	ld.global.nc.f32 	%f8, [%rd17+8];
	add.f32 	%f9, %f8, %f7;
	st.global.f32 	[%rd16+8], %f9;
	ld.global.f32 	%f10, [%rd16+12];
	ld.global.nc.f32 	%f11, [%rd17+12];
	add.f32 	%f12, %f11, %f10;
	st.global.f32 	[%rd16+12], %f12;
	add.s32 	%r16, %r16, 4;
	add.s64 	%rd17, %rd17, 16;
	add.s64 	%rd16, %rd16, 16;
	add.s32 	%r15, %r15, -4;
	setp.ne.s32 	%p3, %r15, 0;
	@%p3 bra 	$L__BB1_3;

$L__BB1_4:
	setp.eq.s32 	%p4, %r17, 0;
	@%p4 bra 	$L__BB1_7;

	mul.wide.s32 	%rd15, %r16, 4;
	add.s64 	%rd19, %rd1, %rd15;
	add.s64 	%rd18, %rd2, %rd15;

$L__BB1_6:
	.pragma "nounroll";
	ld.global.f32 	%f13, [%rd19];
	ld.global.nc.f32 	%f14, [%rd18];
	add.f32 	%f15, %f14, %f13;
	st.global.f32 	[%rd19], %f15;
	add.s64 	%rd19, %rd19, 4;
	add.s64 	%rd18, %rd18, 4;
	add.s32 	%r17, %r17, -1;
	setp.ne.s32 	%p5, %r17, 0;
	@%p5 bra 	$L__BB1_6;

$L__BB1_7:
	ret;

}
	// .globl	_Z19addNaiveSizeTKernelyPKfPf
.visible .entry _Z19addNaiveSizeTKernelyPKfPf(
	.param .u64 _Z19addNaiveSizeTKernelyPKfPf_param_0,
	.param .u64 _Z19addNaiveSizeTKernelyPKfPf_param_1,
	.param .u64 _Z19addNaiveSizeTKernelyPKfPf_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<16>;
	.reg .b64 	%rd<36>;


	ld.param.u64 	%rd21, [_Z19addNaiveSizeTKernelyPKfPf_param_0];
	ld.param.u64 	%rd22, [_Z19addNaiveSizeTKernelyPKfPf_param_1];
	ld.param.u64 	%rd23, [_Z19addNaiveSizeTKernelyPKfPf_param_2];
	cvta.to.global.u64 	%rd1, %rd23;
	cvta.to.global.u64 	%rd2, %rd22;
	setp.eq.s64 	%p1, %rd21, 0;
	@%p1 bra 	$L__BB2_7;

	add.s64 	%rd25, %rd21, -1;
	and.b64  	%rd3, %rd21, 3;
	setp.lt.u64 	%p2, %rd25, 3;
	mov.u64 	%rd32, 0;
	@%p2 bra 	$L__BB2_4;

	sub.s64 	%rd4, %rd3, %rd21;
	mov.u64 	%rd32, 0;
	mov.u64 	%rd29, %rd2;
	mov.u64 	%rd30, %rd1;

$L__BB2_3:
	ld.global.f32 	%f1, [%rd30];
	ld.global.f32 	%f2, [%rd29];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd30], %f3;
	ld.global.f32 	%f4, [%rd30+4];
	ld.global.f32 	%f5, [%rd29+4];
	add.f32 	%f6, %f5, %f4;
	st.global.f32 	[%rd30+4], %f6;
	ld.global.f32 	%f7, [%rd30+8];
	ld.global.f32 	%f8, [%rd29+8];
	add.f32 	%f9, %f8, %f7;
	st.global.f32 	[%rd30+8], %f9;
	ld.global.f32 	%f10, [%rd30+12];
	ld.global.f32 	%f11, [%rd29+12];
	add.f32 	%f12, %f11, %f10;
	st.global.f32 	[%rd30+12], %f12;
	add.s64 	%rd32, %rd32, 4;
	add.s64 	%rd27, %rd4, %rd32;
	add.s64 	%rd30, %rd30, 16;
	add.s64 	%rd29, %rd29, 16;
	setp.ne.s64 	%p3, %rd27, 0;
	@%p3 bra 	$L__BB2_3;

$L__BB2_4:
	setp.eq.s64 	%p4, %rd3, 0;
	@%p4 bra 	$L__BB2_7;

	shl.b64 	%rd28, %rd32, 2;
	add.s64 	%rd35, %rd1, %rd28;
	add.s64 	%rd34, %rd2, %rd28;
	neg.s64 	%rd33, %rd3;

$L__BB2_6:
	.pragma "nounroll";
	ld.global.f32 	%f13, [%rd35];
	ld.global.f32 	%f14, [%rd34];
	add.f32 	%f15, %f14, %f13;
	st.global.f32 	[%rd35], %f15;
	add.s64 	%rd35, %rd35, 4;
	add.s64 	%rd34, %rd34, 4;
	add.s64 	%rd33, %rd33, 1;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB2_6;

$L__BB2_7:
	ret;

}
	// .globl	_Z27addNaiveSizeTRestrictKernelyPKfPf
.visible .entry _Z27addNaiveSizeTRestrictKernelyPKfPf(
	.param .u64 _Z27addNaiveSizeTRestrictKernelyPKfPf_param_0,
	.param .u64 _Z27addNaiveSizeTRestrictKernelyPKfPf_param_1,
	.param .u64 _Z27addNaiveSizeTRestrictKernelyPKfPf_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<16>;
	.reg .b64 	%rd<36>;


	ld.param.u64 	%rd21, [_Z27addNaiveSizeTRestrictKernelyPKfPf_param_0];
	ld.param.u64 	%rd22, [_Z27addNaiveSizeTRestrictKernelyPKfPf_param_1];
	ld.param.u64 	%rd23, [_Z27addNaiveSizeTRestrictKernelyPKfPf_param_2];
	cvta.to.global.u64 	%rd1, %rd23;
	cvta.to.global.u64 	%rd2, %rd22;
	setp.eq.s64 	%p1, %rd21, 0;
	@%p1 bra 	$L__BB3_7;

	add.s64 	%rd25, %rd21, -1;
	and.b64  	%rd3, %rd21, 3;
	setp.lt.u64 	%p2, %rd25, 3;
	mov.u64 	%rd32, 0;
	@%p2 bra 	$L__BB3_4;

	sub.s64 	%rd4, %rd3, %rd21;
	mov.u64 	%rd32, 0;
	mov.u64 	%rd29, %rd2;
	mov.u64 	%rd30, %rd1;

$L__BB3_3:
	ld.global.f32 	%f1, [%rd30];
	ld.global.nc.f32 	%f2, [%rd29];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd30], %f3;
	ld.global.f32 	%f4, [%rd30+4];
	ld.global.nc.f32 	%f5, [%rd29+4];
	add.f32 	%f6, %f5, %f4;
	st.global.f32 	[%rd30+4], %f6;
	ld.global.f32 	%f7, [%rd30+8];
	ld.global.nc.f32 	%f8, [%rd29+8];
	add.f32 	%f9, %f8, %f7;
	st.global.f32 	[%rd30+8], %f9;
	ld.global.f32 	%f10, [%rd30+12];
	ld.global.nc.f32 	%f11, [%rd29+12];
	add.f32 	%f12, %f11, %f10;
	st.global.f32 	[%rd30+12], %f12;
	add.s64 	%rd32, %rd32, 4;
	add.s64 	%rd27, %rd4, %rd32;
	add.s64 	%rd30, %rd30, 16;
	add.s64 	%rd29, %rd29, 16;
	setp.ne.s64 	%p3, %rd27, 0;
	@%p3 bra 	$L__BB3_3;

$L__BB3_4:
	setp.eq.s64 	%p4, %rd3, 0;
	@%p4 bra 	$L__BB3_7;

	shl.b64 	%rd28, %rd32, 2;
	add.s64 	%rd35, %rd1, %rd28;
	add.s64 	%rd34, %rd2, %rd28;
	neg.s64 	%rd33, %rd3;

$L__BB3_6:
	.pragma "nounroll";
	ld.global.f32 	%f13, [%rd35];
	ld.global.nc.f32 	%f14, [%rd34];
	add.f32 	%f15, %f14, %f13;
	st.global.f32 	[%rd35], %f15;
	add.s64 	%rd35, %rd35, 4;
	add.s64 	%rd34, %rd34, 4;
	add.s64 	%rd33, %rd33, 1;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB3_6;

$L__BB3_7:
	ret;

}
	// .globl	_Z28addNaiveFloat2RestrictKerneliPKfPf
.visible .entry _Z28addNaiveFloat2RestrictKerneliPKfPf(
	.param .u32 _Z28addNaiveFloat2RestrictKerneliPKfPf_param_0,
	.param .u64 _Z28addNaiveFloat2RestrictKerneliPKfPf_param_1,
	.param .u64 _Z28addNaiveFloat2RestrictKerneliPKfPf_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<54>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<24>;


	ld.param.u32 	%r11, [_Z28addNaiveFloat2RestrictKerneliPKfPf_param_0];
	ld.param.u64 	%rd13, [_Z28addNaiveFloat2RestrictKerneliPKfPf_param_1];
	ld.param.u64 	%rd14, [_Z28addNaiveFloat2RestrictKerneliPKfPf_param_2];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	setp.lt.s32 	%p1, %r11, 2;
	@%p1 bra 	$L__BB4_7;

	shr.s32 	%r13, %r11, 1;
	max.s32 	%r1, %r13, 1;
	add.s32 	%r14, %r1, -1;
	and.b32  	%r21, %r1, 3;
	setp.lt.u32 	%p2, %r14, 3;
	mov.u32 	%r20, 0;
	@%p2 bra 	$L__BB4_4;

	sub.s32 	%r19, %r1, %r21;
	mov.u32 	%r20, 0;
	mov.u64 	%rd20, %rd2;
	mov.u64 	%rd21, %rd1;

$L__BB4_3:
	ld.global.nc.v2.f32 	{%f1, %f2}, [%rd20];
	ld.global.v2.f32 	{%f5, %f6}, [%rd21];
	add.f32 	%f9, %f2, %f6;
	add.f32 	%f10, %f1, %f5;
	st.global.v2.f32 	[%rd21], {%f10, %f9};
	ld.global.nc.v2.f32 	{%f11, %f12}, [%rd20+8];
	ld.global.v2.f32 	{%f15, %f16}, [%rd21+8];
	add.f32 	%f19, %f12, %f16;
	add.f32 	%f20, %f11, %f15;
	st.global.v2.f32 	[%rd21+8], {%f20, %f19};
	ld.global.nc.v2.f32 	{%f21, %f22}, [%rd20+16];
	ld.global.v2.f32 	{%f25, %f26}, [%rd21+16];
	add.f32 	%f29, %f22, %f26;
	add.f32 	%f30, %f21, %f25;
	st.global.v2.f32 	[%rd21+16], {%f30, %f29};
	ld.global.nc.v2.f32 	{%f31, %f32}, [%rd20+24];
	ld.global.v2.f32 	{%f35, %f36}, [%rd21+24];
	add.f32 	%f39, %f32, %f36;
	add.f32 	%f40, %f31, %f35;
	st.global.v2.f32 	[%rd21+24], {%f40, %f39};
	add.s32 	%r20, %r20, 4;
	add.s64 	%rd21, %rd21, 32;
	add.s64 	%rd20, %rd20, 32;
	add.s32 	%r19, %r19, -4;
	setp.ne.s32 	%p3, %r19, 0;
	@%p3 bra 	$L__BB4_3;

$L__BB4_4:
	setp.eq.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB4_7;

	mul.wide.s32 	%rd15, %r20, 2;
	shl.b64 	%rd16, %rd15, 2;
	add.s64 	%rd23, %rd2, %rd16;
	add.s64 	%rd22, %rd1, %rd16;

$L__BB4_6:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd23];
	ld.global.v2.f32 	{%f45, %f46}, [%rd22];
	add.f32 	%f49, %f42, %f46;
	add.f32 	%f50, %f41, %f45;
	st.global.v2.f32 	[%rd22], {%f50, %f49};
	add.s64 	%rd23, %rd23, 8;
	add.s64 	%rd22, %rd22, 8;
	add.s32 	%r21, %r21, -1;
	setp.ne.s32 	%p5, %r21, 0;
	@%p5 bra 	$L__BB4_6;

$L__BB4_7:
	and.b32  	%r16, %r11, 1;
	setp.eq.b32 	%p6, %r16, 1;
	mov.pred 	%p7, 0;
	xor.pred  	%p8, %p6, %p7;
	not.pred 	%p9, %p8;
	@%p9 bra 	$L__BB4_9;

	add.s32 	%r17, %r11, -1;
	mul.wide.s32 	%rd17, %r17, 4;
	add.s64 	%rd18, %rd2, %rd17;
	add.s64 	%rd19, %rd1, %rd17;
	ld.global.f32 	%f51, [%rd19];
	ld.global.nc.f32 	%f52, [%rd18];
	add.f32 	%f53, %f52, %f51;
	st.global.f32 	[%rd19], %f53;

$L__BB4_9:
	ret;

}
	// .globl	_Z34addNaiveFloat2RestrictNoTailKerneliPK6float2PS_
.visible .entry _Z34addNaiveFloat2RestrictNoTailKerneliPK6float2PS_(
	.param .u32 _Z34addNaiveFloat2RestrictNoTailKerneliPK6float2PS__param_0,
	.param .u64 _Z34addNaiveFloat2RestrictNoTailKerneliPK6float2PS__param_1,
	.param .u64 _Z34addNaiveFloat2RestrictNoTailKerneliPK6float2PS__param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<51>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<20>;


	ld.param.u32 	%r10, [_Z34addNaiveFloat2RestrictNoTailKerneliPK6float2PS__param_0];
	ld.param.u64 	%rd13, [_Z34addNaiveFloat2RestrictNoTailKerneliPK6float2PS__param_1];
	ld.param.u64 	%rd14, [_Z34addNaiveFloat2RestrictNoTailKerneliPK6float2PS__param_2];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	setp.lt.s32 	%p1, %r10, 1;
	@%p1 bra 	$L__BB5_7;

	add.s32 	%r12, %r10, -1;
	and.b32  	%r17, %r10, 3;
	setp.lt.u32 	%p2, %r12, 3;
	mov.u32 	%r16, 0;
	@%p2 bra 	$L__BB5_4;

	sub.s32 	%r15, %r10, %r17;
	mov.u32 	%r16, 0;
	mov.u64 	%rd16, %rd2;
	mov.u64 	%rd17, %rd1;

$L__BB5_3:
	ld.global.nc.v2.f32 	{%f1, %f2}, [%rd16];
	ld.global.v2.f32 	{%f5, %f6}, [%rd17];
	add.f32 	%f9, %f2, %f6;
	add.f32 	%f10, %f1, %f5;
	st.global.v2.f32 	[%rd17], {%f10, %f9};
	ld.global.nc.v2.f32 	{%f11, %f12}, [%rd16+8];
	ld.global.v2.f32 	{%f15, %f16}, [%rd17+8];
	add.f32 	%f19, %f12, %f16;
	add.f32 	%f20, %f11, %f15;
	st.global.v2.f32 	[%rd17+8], {%f20, %f19};
	ld.global.nc.v2.f32 	{%f21, %f22}, [%rd16+16];
	ld.global.v2.f32 	{%f25, %f26}, [%rd17+16];
	add.f32 	%f29, %f22, %f26;
	add.f32 	%f30, %f21, %f25;
	st.global.v2.f32 	[%rd17+16], {%f30, %f29};
	ld.global.nc.v2.f32 	{%f31, %f32}, [%rd16+24];
	ld.global.v2.f32 	{%f35, %f36}, [%rd17+24];
	add.f32 	%f39, %f32, %f36;
	add.f32 	%f40, %f31, %f35;
	st.global.v2.f32 	[%rd17+24], {%f40, %f39};
	add.s32 	%r16, %r16, 4;
	add.s64 	%rd17, %rd17, 32;
	add.s64 	%rd16, %rd16, 32;
	add.s32 	%r15, %r15, -4;
	setp.ne.s32 	%p3, %r15, 0;
	@%p3 bra 	$L__BB5_3;

$L__BB5_4:
	setp.eq.s32 	%p4, %r17, 0;
	@%p4 bra 	$L__BB5_7;

	mul.wide.s32 	%rd15, %r16, 8;
	add.s64 	%rd19, %rd2, %rd15;
	add.s64 	%rd18, %rd1, %rd15;

$L__BB5_6:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd19];
	ld.global.v2.f32 	{%f45, %f46}, [%rd18];
	add.f32 	%f49, %f42, %f46;
	add.f32 	%f50, %f41, %f45;
	st.global.v2.f32 	[%rd18], {%f50, %f49};
	add.s64 	%rd19, %rd19, 8;
	add.s64 	%rd18, %rd18, 8;
	add.s32 	%r17, %r17, -1;
	setp.ne.s32 	%p5, %r17, 0;
	@%p5 bra 	$L__BB5_6;

$L__BB5_7:
	ret;

}
	// .globl	_Z28addNaiveFloat4RestrictKerneliPKfPf
.visible .entry _Z28addNaiveFloat4RestrictKerneliPKfPf(
	.param .u32 _Z28addNaiveFloat4RestrictKerneliPKfPf_param_0,
	.param .u64 _Z28addNaiveFloat4RestrictKerneliPKfPf_param_1,
	.param .u64 _Z28addNaiveFloat4RestrictKerneliPKfPf_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<116>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<40>;


	ld.param.u32 	%r20, [_Z28addNaiveFloat4RestrictKerneliPKfPf_param_0];
	ld.param.u64 	%rd25, [_Z28addNaiveFloat4RestrictKerneliPKfPf_param_1];
	ld.param.u64 	%rd26, [_Z28addNaiveFloat4RestrictKerneliPKfPf_param_2];
	cvta.to.global.u64 	%rd1, %rd26;
	cvta.to.global.u64 	%rd2, %rd25;
	setp.lt.s32 	%p1, %r20, 4;
	@%p1 bra 	$L__BB6_7;

	shr.s32 	%r22, %r20, 2;
	max.s32 	%r1, %r22, 1;
	add.s32 	%r23, %r1, -1;
	and.b32  	%r30, %r1, 3;
	setp.lt.u32 	%p2, %r23, 3;
	mov.u32 	%r29, 0;
	@%p2 bra 	$L__BB6_4;

	sub.s32 	%r28, %r1, %r30;
	mov.u32 	%r29, 0;
	mov.u64 	%rd32, %rd2;
	mov.u64 	%rd33, %rd1;

$L__BB6_3:
	ld.global.nc.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd32];
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd33];
	add.f32 	%f17, %f4, %f12;
	add.f32 	%f18, %f3, %f11;
	add.f32 	%f19, %f2, %f10;
	add.f32 	%f20, %f1, %f9;
	st.global.v4.f32 	[%rd33], {%f20, %f19, %f18, %f17};
	ld.global.nc.v4.f32 	{%f21, %f22, %f23, %f24}, [%rd32+16];
	ld.global.v4.f32 	{%f29, %f30, %f31, %f32}, [%rd33+16];
	add.f32 	%f37, %f24, %f32;
	add.f32 	%f38, %f23, %f31;
	add.f32 	%f39, %f22, %f30;
	add.f32 	%f40, %f21, %f29;
	st.global.v4.f32 	[%rd33+16], {%f40, %f39, %f38, %f37};
	ld.global.nc.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd32+32];
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd33+32];
	add.f32 	%f57, %f44, %f52;
	add.f32 	%f58, %f43, %f51;
	add.f32 	%f59, %f42, %f50;
	add.f32 	%f60, %f41, %f49;
	st.global.v4.f32 	[%rd33+32], {%f60, %f59, %f58, %f57};
	ld.global.nc.v4.f32 	{%f61, %f62, %f63, %f64}, [%rd32+48];
	ld.global.v4.f32 	{%f69, %f70, %f71, %f72}, [%rd33+48];
	add.f32 	%f77, %f64, %f72;
	add.f32 	%f78, %f63, %f71;
	add.f32 	%f79, %f62, %f70;
	add.f32 	%f80, %f61, %f69;
	st.global.v4.f32 	[%rd33+48], {%f80, %f79, %f78, %f77};
	add.s32 	%r29, %r29, 4;
	add.s64 	%rd33, %rd33, 64;
	add.s64 	%rd32, %rd32, 64;
	add.s32 	%r28, %r28, -4;
	setp.ne.s32 	%p3, %r28, 0;
	@%p3 bra 	$L__BB6_3;

$L__BB6_4:
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB6_7;

	mul.wide.s32 	%rd27, %r29, 4;
	shl.b64 	%rd28, %rd27, 2;
	add.s64 	%rd35, %rd2, %rd28;
	add.s64 	%rd34, %rd1, %rd28;

$L__BB6_6:
	.pragma "nounroll";
	ld.global.nc.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd35];
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd34];
	add.f32 	%f97, %f84, %f92;
	add.f32 	%f98, %f83, %f91;
	add.f32 	%f99, %f82, %f90;
	add.f32 	%f100, %f81, %f89;
	st.global.v4.f32 	[%rd34], {%f100, %f99, %f98, %f97};
	add.s64 	%rd35, %rd35, 16;
	add.s64 	%rd34, %rd34, 16;
	add.s32 	%r30, %r30, -1;
	setp.ne.s32 	%p5, %r30, 0;
	@%p5 bra 	$L__BB6_6;

$L__BB6_7:
	and.b32  	%r11, %r20, -4;
	setp.ge.s32 	%p6, %r11, %r20;
	@%p6 bra 	$L__BB6_14;

	and.b32  	%r32, %r20, 3;
	setp.eq.s32 	%p7, %r32, 0;
	mov.u32 	%r33, %r11;
	@%p7 bra 	$L__BB6_11;

	mul.wide.s32 	%rd29, %r11, 4;
	add.s64 	%rd37, %rd1, %rd29;
	add.s64 	%rd36, %rd2, %rd29;
	mov.u32 	%r33, %r11;

$L__BB6_10:
	.pragma "nounroll";
	ld.global.f32 	%f101, [%rd37];
	ld.global.nc.f32 	%f102, [%rd36];
	add.f32 	%f103, %f102, %f101;
	st.global.f32 	[%rd37], %f103;
	add.s32 	%r33, %r33, 1;
	add.s64 	%rd37, %rd37, 4;
	add.s64 	%rd36, %rd36, 4;
	add.s32 	%r32, %r32, -1;
	setp.ne.s32 	%p8, %r32, 0;
	@%p8 bra 	$L__BB6_10;

$L__BB6_11:
	not.b32 	%r25, %r11;
	add.s32 	%r26, %r25, %r20;
	setp.lt.u32 	%p9, %r26, 3;
	@%p9 bra 	$L__BB6_14;

	mul.wide.s32 	%rd30, %r33, 4;
	add.s64 	%rd31, %rd30, 8;
	add.s64 	%rd39, %rd2, %rd31;
	add.s64 	%rd38, %rd1, %rd31;

$L__BB6_13:
	ld.global.f32 	%f104, [%rd38+-8];
	ld.global.nc.f32 	%f105, [%rd39+-8];
	add.f32 	%f106, %f105, %f104;
	st.global.f32 	[%rd38+-8], %f106;
	ld.global.f32 	%f107, [%rd38+-4];
	ld.global.nc.f32 	%f108, [%rd39+-4];
	add.f32 	%f109, %f108, %f107;
	st.global.f32 	[%rd38+-4], %f109;
	ld.global.f32 	%f110, [%rd38];
	ld.global.nc.f32 	%f111, [%rd39];
	add.f32 	%f112, %f111, %f110;
	st.global.f32 	[%rd38], %f112;
	ld.global.f32 	%f113, [%rd38+4];
	ld.global.nc.f32 	%f114, [%rd39+4];
	add.f32 	%f115, %f114, %f113;
	st.global.f32 	[%rd38+4], %f115;
	add.s64 	%rd39, %rd39, 16;
	add.s64 	%rd38, %rd38, 16;
	add.s32 	%r33, %r33, 4;
	setp.lt.s32 	%p10, %r33, %r20;
	@%p10 bra 	$L__BB6_13;

$L__BB6_14:
	ret;

}
	// .globl	_Z26addNaiveFloat4NoTailKerneliPK6float4PS_
.visible .entry _Z26addNaiveFloat4NoTailKerneliPK6float4PS_(
	.param .u32 _Z26addNaiveFloat4NoTailKerneliPK6float4PS__param_0,
	.param .u64 _Z26addNaiveFloat4NoTailKerneliPK6float4PS__param_1,
	.param .u64 _Z26addNaiveFloat4NoTailKerneliPK6float4PS__param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<101>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<20>;


	ld.param.u32 	%r10, [_Z26addNaiveFloat4NoTailKerneliPK6float4PS__param_0];
	ld.param.u64 	%rd13, [_Z26addNaiveFloat4NoTailKerneliPK6float4PS__param_1];
	ld.param.u64 	%rd14, [_Z26addNaiveFloat4NoTailKerneliPK6float4PS__param_2];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	setp.lt.s32 	%p1, %r10, 1;
	@%p1 bra 	$L__BB7_7;

	add.s32 	%r12, %r10, -1;
	and.b32  	%r17, %r10, 3;
	setp.lt.u32 	%p2, %r12, 3;
	mov.u32 	%r16, 0;
	@%p2 bra 	$L__BB7_4;

	sub.s32 	%r15, %r10, %r17;
	mov.u32 	%r16, 0;
	mov.u64 	%rd16, %rd2;
	mov.u64 	%rd17, %rd1;

$L__BB7_3:
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd16];
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd17];
	add.f32 	%f17, %f4, %f12;
	add.f32 	%f18, %f3, %f11;
	add.f32 	%f19, %f2, %f10;
	add.f32 	%f20, %f1, %f9;
	st.global.v4.f32 	[%rd17], {%f20, %f19, %f18, %f17};
	ld.global.v4.f32 	{%f21, %f22, %f23, %f24}, [%rd16+16];
	ld.global.v4.f32 	{%f29, %f30, %f31, %f32}, [%rd17+16];
	add.f32 	%f37, %f24, %f32;
	add.f32 	%f38, %f23, %f31;
	add.f32 	%f39, %f22, %f30;
	add.f32 	%f40, %f21, %f29;
	st.global.v4.f32 	[%rd17+16], {%f40, %f39, %f38, %f37};
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd16+32];
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd17+32];
	add.f32 	%f57, %f44, %f52;
	add.f32 	%f58, %f43, %f51;
	add.f32 	%f59, %f42, %f50;
	add.f32 	%f60, %f41, %f49;
	st.global.v4.f32 	[%rd17+32], {%f60, %f59, %f58, %f57};
	ld.global.v4.f32 	{%f61, %f62, %f63, %f64}, [%rd16+48];
	ld.global.v4.f32 	{%f69, %f70, %f71, %f72}, [%rd17+48];
	add.f32 	%f77, %f64, %f72;
	add.f32 	%f78, %f63, %f71;
	add.f32 	%f79, %f62, %f70;
	add.f32 	%f80, %f61, %f69;
	st.global.v4.f32 	[%rd17+48], {%f80, %f79, %f78, %f77};
	add.s32 	%r16, %r16, 4;
	add.s64 	%rd17, %rd17, 64;
	add.s64 	%rd16, %rd16, 64;
	add.s32 	%r15, %r15, -4;
	setp.ne.s32 	%p3, %r15, 0;
	@%p3 bra 	$L__BB7_3;

$L__BB7_4:
	setp.eq.s32 	%p4, %r17, 0;
	@%p4 bra 	$L__BB7_7;

	mul.wide.s32 	%rd15, %r16, 16;
	add.s64 	%rd19, %rd2, %rd15;
	add.s64 	%rd18, %rd1, %rd15;

$L__BB7_6:
	.pragma "nounroll";
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd19];
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd18];
	add.f32 	%f97, %f84, %f92;
	add.f32 	%f98, %f83, %f91;
	add.f32 	%f99, %f82, %f90;
	add.f32 	%f100, %f81, %f89;
	st.global.v4.f32 	[%rd18], {%f100, %f99, %f98, %f97};
	add.s64 	%rd19, %rd19, 16;
	add.s64 	%rd18, %rd18, 16;
	add.s32 	%r17, %r17, -1;
	setp.ne.s32 	%p5, %r17, 0;
	@%p5 bra 	$L__BB7_6;

$L__BB7_7:
	ret;

}
	// .globl	_Z34addNaiveFloat4RestrictNoTailKerneliPK6float4PS_
.visible .entry _Z34addNaiveFloat4RestrictNoTailKerneliPK6float4PS_(
	.param .u32 _Z34addNaiveFloat4RestrictNoTailKerneliPK6float4PS__param_0,
	.param .u64 _Z34addNaiveFloat4RestrictNoTailKerneliPK6float4PS__param_1,
	.param .u64 _Z34addNaiveFloat4RestrictNoTailKerneliPK6float4PS__param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<101>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<20>;


	ld.param.u32 	%r10, [_Z34addNaiveFloat4RestrictNoTailKerneliPK6float4PS__param_0];
	ld.param.u64 	%rd13, [_Z34addNaiveFloat4RestrictNoTailKerneliPK6float4PS__param_1];
	ld.param.u64 	%rd14, [_Z34addNaiveFloat4RestrictNoTailKerneliPK6float4PS__param_2];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	setp.lt.s32 	%p1, %r10, 1;
	@%p1 bra 	$L__BB8_7;

	add.s32 	%r12, %r10, -1;
	and.b32  	%r17, %r10, 3;
	setp.lt.u32 	%p2, %r12, 3;
	mov.u32 	%r16, 0;
	@%p2 bra 	$L__BB8_4;

	sub.s32 	%r15, %r10, %r17;
	mov.u32 	%r16, 0;
	mov.u64 	%rd16, %rd2;
	mov.u64 	%rd17, %rd1;

$L__BB8_3:
	ld.global.nc.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd16];
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd17];
	add.f32 	%f17, %f4, %f12;
	add.f32 	%f18, %f3, %f11;
	add.f32 	%f19, %f2, %f10;
	add.f32 	%f20, %f1, %f9;
	st.global.v4.f32 	[%rd17], {%f20, %f19, %f18, %f17};
	ld.global.nc.v4.f32 	{%f21, %f22, %f23, %f24}, [%rd16+16];
	ld.global.v4.f32 	{%f29, %f30, %f31, %f32}, [%rd17+16];
	add.f32 	%f37, %f24, %f32;
	add.f32 	%f38, %f23, %f31;
	add.f32 	%f39, %f22, %f30;
	add.f32 	%f40, %f21, %f29;
	st.global.v4.f32 	[%rd17+16], {%f40, %f39, %f38, %f37};
	ld.global.nc.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd16+32];
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd17+32];
	add.f32 	%f57, %f44, %f52;
	add.f32 	%f58, %f43, %f51;
	add.f32 	%f59, %f42, %f50;
	add.f32 	%f60, %f41, %f49;
	st.global.v4.f32 	[%rd17+32], {%f60, %f59, %f58, %f57};
	ld.global.nc.v4.f32 	{%f61, %f62, %f63, %f64}, [%rd16+48];
	ld.global.v4.f32 	{%f69, %f70, %f71, %f72}, [%rd17+48];
	add.f32 	%f77, %f64, %f72;
	add.f32 	%f78, %f63, %f71;
	add.f32 	%f79, %f62, %f70;
	add.f32 	%f80, %f61, %f69;
	st.global.v4.f32 	[%rd17+48], {%f80, %f79, %f78, %f77};
	add.s32 	%r16, %r16, 4;
	add.s64 	%rd17, %rd17, 64;
	add.s64 	%rd16, %rd16, 64;
	add.s32 	%r15, %r15, -4;
	setp.ne.s32 	%p3, %r15, 0;
	@%p3 bra 	$L__BB8_3;

$L__BB8_4:
	setp.eq.s32 	%p4, %r17, 0;
	@%p4 bra 	$L__BB8_7;

	mul.wide.s32 	%rd15, %r16, 16;
	add.s64 	%rd19, %rd2, %rd15;
	add.s64 	%rd18, %rd1, %rd15;

$L__BB8_6:
	.pragma "nounroll";
	ld.global.nc.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd19];
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd18];
	add.f32 	%f97, %f84, %f92;
	add.f32 	%f98, %f83, %f91;
	add.f32 	%f99, %f82, %f90;
	add.f32 	%f100, %f81, %f89;
	st.global.v4.f32 	[%rd18], {%f100, %f99, %f98, %f97};
	add.s64 	%rd19, %rd19, 16;
	add.s64 	%rd18, %rd18, 16;
	add.s32 	%r17, %r17, -1;
	setp.ne.s32 	%p5, %r17, 0;
	@%p5 bra 	$L__BB8_6;

$L__BB8_7:
	ret;

}

